{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5dac39b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import json\n",
    "import os\n",
    "import mp\n",
    "import re\n",
    "import subprocess\n",
    "import soundfile as sf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c614bdc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22492"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files = glob('malaysian-podcast_processed/**/*/*.json', recursive = True)\n",
    "files.extend(glob('/home/husein/ssd3/sg-podcast_processed/**/*/*.json', recursive = True))\n",
    "\n",
    "len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0378019",
   "metadata": {},
   "outputs": [],
   "source": [
    "rejected = [\n",
    "    'terima kasih kerana menonton',\n",
    "    'terima kasih',\n",
    "    'thank you for watching',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "067b6cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = -18\n",
    "\n",
    "def new_path(f):\n",
    "    f = f.replace('.mp3', '.alignment')\n",
    "    f = f.replace('_processed/', '_processed_alignment/')\n",
    "    return f\n",
    "\n",
    "def new_path_lang(f):\n",
    "    f = f.replace('.mp3', '.language')\n",
    "    f = f.replace('_processed/', '_processed_language/')\n",
    "    return f\n",
    "\n",
    "def chunk(alignment, reject = threshold, minimum_length = 2.0):\n",
    "    alls, temp = [], []\n",
    "    for a in alignment:\n",
    "        if a['score'] <= reject:\n",
    "            if len(temp):\n",
    "                temp[-1]['end'] = a['start']\n",
    "                if (temp[-1]['end'] - temp[0]['start']) >= minimum_length:\n",
    "                    alls.append(temp)\n",
    "                temp = []\n",
    "        else:\n",
    "            temp.append(a)\n",
    "            \n",
    "    if len(temp):\n",
    "        if (temp[-1]['end'] - temp[0]['start']) >= minimum_length:\n",
    "            alls.append(temp)\n",
    "    return alls\n",
    "        \n",
    "def clean(string):\n",
    "    string = re.sub('[^A-Za-z ]+', ' ', string)\n",
    "    string = re.sub(r'[ ]+', ' ', string).strip()\n",
    "    return string.lower()\n",
    "\n",
    "def detect_extra(word):\n",
    "    word = clean(word)\n",
    "    return word in [\n",
    "        'eh', 'ehh', 'oh', 'ohh', 'uhm', 'uhmm',\n",
    "        'um', 'em', 'emm', 'ah', 'ha', 'ok', 'okay',\n",
    "        'so', 'yes', 'no', 'ah', 'aa', 'so', 'uh', 'um', 'eh', 'ha', 'oh', 'ye', 'haa',\n",
    "        'oi', 'ya', 'leh', 'lah', 'haiya', 'hoi', 'haha', 'hahaha',\n",
    "        'then', 'it s'\n",
    "    ]\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "def generate_trigrams(text):\n",
    "    words = text.split()\n",
    "    return list(zip(words, words[1:], words[2:]))\n",
    "\n",
    "def skip_trigrams(text):\n",
    "    trigrams = generate_trigrams(text)\n",
    "    count = defaultdict(int)\n",
    "    total = 0\n",
    "    for t in trigrams:\n",
    "        count[''.join(t)] += 1\n",
    "        total += 1\n",
    "    if len(count.keys()) < 3:\n",
    "        return True\n",
    "    for k, v in count.items():\n",
    "        if (v / total) > 0.2:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e14877bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf podcast-chunk\n",
    "!mkdir podcast-chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3332d699",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loop(files):\n",
    "    files, _ = files\n",
    "    data = []\n",
    "    for file in tqdm(files):\n",
    "        folder = os.path.split(file)[0]\n",
    "        folder_folder = os.path.split(folder)[1]\n",
    "        filename = file.replace('.json', '')\n",
    "\n",
    "        try:\n",
    "            with open(file) as fopen:\n",
    "                d = json.load(fopen)\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "        for no, obj in enumerate(d):\n",
    "            text = obj[\"text\"].strip()\n",
    "            \n",
    "            rt_ = re.sub('[^a-z ]+', '', text.lower()).strip()\n",
    "            if any([s == rt_ for s in rejected]):\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                dense = CountVectorizer(ngram_range = (3,3)).fit_transform([text]).todense()\n",
    "                repeat = (dense > 3).sum() >= 1\n",
    "                if repeat:\n",
    "                    continue\n",
    "            except:\n",
    "                continue\n",
    "            \n",
    "            audio_path = os.path.join(folder, f'{folder_folder}_{no}.mp3')\n",
    "            \n",
    "            if not os.path.exists(audio_path):\n",
    "                continue\n",
    "                \n",
    "            align_path = new_path(audio_path)\n",
    "            \n",
    "            if not os.path.exists(align_path):\n",
    "                continue\n",
    "                \n",
    "            with open(align_path) as fopen:\n",
    "                align = json.load(fopen)\n",
    "                \n",
    "            for a in align:\n",
    "                if detect_extra(a['text']):\n",
    "                    a['score'] = 0.0\n",
    "                \n",
    "            scores = [a for a in align if a['score'] <= threshold]\n",
    "            if not len(scores):\n",
    "                continue\n",
    "            \n",
    "            chunks = chunk(align)\n",
    "            if len(chunks):\n",
    "                y, sr = sf.read(audio_path)\n",
    "                for no, c in enumerate(chunks):\n",
    "                    if len(c) == len(align):\n",
    "                        continue\n",
    "                    try:\n",
    "                        \n",
    "                        words = [c_['text'] for c_ in c if len(c_['text']) <= 1]\n",
    "                        if (len(words) / len(c)) > 0.5:\n",
    "                            print(c)\n",
    "                            continue\n",
    "                        \n",
    "                        skip = False\n",
    "                        \n",
    "                        for c_ in c:\n",
    "                            if (c_['end'] - c_['start']) >= 2:\n",
    "                                skip = True\n",
    "                                break\n",
    "                        if skip:\n",
    "                            continue\n",
    "                        \n",
    "                        for i in range(1, len(c), 1):\n",
    "                            if (c[i]['start'] - c[i - 1]['end']) >= 2:\n",
    "                                skip = True\n",
    "                                break\n",
    "                        if skip:\n",
    "                            continue\n",
    "                            \n",
    "                        t = ' '.join([c_['text'] for c_ in c])\n",
    "                        start = c[0]['start']\n",
    "                        end = c[-1]['end']\n",
    "                        \n",
    "                        a = audio_path.replace('/', '_').replace('.mp3', '') \n",
    "                        a = os.path.join('podcast-chunk', f'{a}_{no}.mp3')\n",
    "                        if not os.path.exists(a):\n",
    "                            sf.write(a, y[int(sr * start): int(sr * end)], sr)\n",
    "\n",
    "                        data.append({\n",
    "                            'audio': a,\n",
    "                            'transcription': t,\n",
    "                        })\n",
    "                    except:\n",
    "                        pass\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "96b87089",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 2334.06it/s]\n"
     ]
    }
   ],
   "source": [
    "d = loop((files[-1:], 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7605643e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b91abda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython.display as ipd\n",
    "ipd.Audio(d[-1]['audio'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e054b55a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|█████████████████▏                                                                    | 299/1499 [03:53<56:16,  2.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'start': 0.08, 'end': 0.08, 'text': 'I', 'score': 0.0}, {'start': 0.14, 'end': 0.14, 'text': 'P', 'score': 0.0}, {'start': 0.3, 'end': 0.3, 'text': 'T', 'score': 0.0}, {'start': 0.54, 'end': 0.54, 'text': 'A', 'score': 0.0}, {'start': 0.7, 'end': 0.86, 'text': 'atau', 'score': -0.5529270172119141}, {'start': 1.02, 'end': 1.02, 'text': 'I', 'score': 0.0}, {'start': 1.1, 'end': 1.1, 'text': 'P', 'score': 0.0}, {'start': 1.22, 'end': 1.22, 'text': 'T', 'score': 0.0}, {'start': 1.56, 'end': 1.56, 'text': 'S', 'score': 0.0}, {'start': 1.8, 'end': 1.96, 'text': 'untuk', 'score': -0.2971644401550293}, {'start': 2.02, 'end': 2.22, 'text': 'pulang', 'score': -0.07382869720458984}, {'start': 2.26, 'end': 2.3, 'text': 'ke', 'score': -0.020725250244140625}, {'start': 2.38, 'end': 2.76, 'text': 'kampung', 'score': -0.22738373279571533}, {'start': 2.86, 'end': 2.96, 'text': 'dan', 'score': -0.011804580688476562}, {'start': 3.14, 'end': 3.94, 'text': 'berkemungkinan', 'score': -1.3972400426864624}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 1499/1499 [16:39<00:00,  1.50it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:00<00:00, 21.04it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 1499/1499 [16:40<00:00,  1.50it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 1499/1499 [16:46<00:00,  1.49it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 1499/1499 [17:06<00:00,  1.46it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 1499/1499 [17:15<00:00,  1.45it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 1499/1499 [17:36<00:00,  1.42it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 1499/1499 [17:55<00:00,  1.39it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 1499/1499 [18:00<00:00,  1.39it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 1499/1499 [18:04<00:00,  1.38it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 1499/1499 [18:04<00:00,  1.38it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 1499/1499 [18:38<00:00,  1.34it/s]\n",
      " 89%|███████████████████████████████████████████████████████████████████████████▌         | 1332/1499 [18:41<00:35,  4.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'start': 0.02, 'end': 0.02, 'text': 'I', 'score': 0.0}, {'start': 0.1, 'end': 0.24, 'text': 'think,', 'score': -17.1785888671875}, {'start': 1.66, 'end': 2.02, 'text': 'I', 'score': 0.0}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 1499/1499 [18:45<00:00,  1.33it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 1499/1499 [19:23<00:00,  1.29it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 1499/1499 [20:51<00:00,  1.20it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 1499/1499 [21:31<00:00,  1.16it/s]\n"
     ]
    }
   ],
   "source": [
    "data = mp.multiprocessing(files, loop, cores = 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7efac780",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "205831"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8d711233",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 205831/205831 [00:00<00:00, 3081890.96it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'podcast-chunk/malaysian-podcast': 166913,\n",
       "             'podcast-chunk/_home_husein_ssd3_sg-podcast': 38918})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "uniques = defaultdict(int)\n",
    "for d in tqdm(data):\n",
    "    uniques[d['audio'].split('_processed')[0]] += 1\n",
    "    \n",
    "uniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5e5a20a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>audio</th>\n",
       "      <th>transcription</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>podcast-chunk/malaysian-podcast_processed_3 Te...</td>\n",
       "      <td>Ada satu pepatah dalam MMA ni kata, don't give...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>podcast-chunk/malaysian-podcast_processed_3 Te...</td>\n",
       "      <td>di dalam, you know, sukan grappling ataupun MM...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>podcast-chunk/malaysian-podcast_processed_3 Te...</td>\n",
       "      <td>Tapi yang common orang buat ni biasa, rear nak...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>podcast-chunk/malaysian-podcast_processed_3 Te...</td>\n",
       "      <td>Rear Naked Choke kot. Sebab, bila opponent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>podcast-chunk/malaysian-podcast_processed_3 Te...</td>\n",
       "      <td>tu berada di belakang apa, posisi, maksudnya b...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               audio  \\\n",
       "0  podcast-chunk/malaysian-podcast_processed_3 Te...   \n",
       "1  podcast-chunk/malaysian-podcast_processed_3 Te...   \n",
       "2  podcast-chunk/malaysian-podcast_processed_3 Te...   \n",
       "3  podcast-chunk/malaysian-podcast_processed_3 Te...   \n",
       "4  podcast-chunk/malaysian-podcast_processed_3 Te...   \n",
       "\n",
       "                                       transcription  \n",
       "0  Ada satu pepatah dalam MMA ni kata, don't give...  \n",
       "1  di dalam, you know, sukan grappling ataupun MM...  \n",
       "2  Tapi yang common orang buat ni biasa, rear nak...  \n",
       "3         Rear Naked Choke kot. Sebab, bila opponent  \n",
       "4  tu berada di belakang apa, posisi, maksudnya b...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8f4993ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_parquet('verify-text-chunk-podcasts.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a0dff8da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.1G\tnew_chunk\r\n"
     ]
    }
   ],
   "source": [
    "!du -hs new_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9b4ff018",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !zip -rq text-chunk-podcasts.zip new_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5774ef42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1785a58fe2dd454f8dcc621a5ddbc688",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "verify-text-chunk-podcasts.parquet:   0%|          | 0.00/16.6M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/mesolitica/Malaysian-Voice-Conversion/commit/c5a7937e39dff59f1233e47ece68de758add7269', commit_message='Upload data/podcasts_chunk-00000-of-00001.parquet with huggingface_hub', commit_description='', oid='c5a7937e39dff59f1233e47ece68de758add7269', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/mesolitica/Malaysian-Voice-Conversion', endpoint='https://huggingface.co', repo_type='dataset', repo_id='mesolitica/Malaysian-Voice-Conversion'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import HfApi\n",
    "api = HfApi()\n",
    "api.upload_file(\n",
    "    path_or_fileobj=\"verify-text-chunk-podcasts.parquet\",\n",
    "    path_in_repo=\"data/podcasts_chunk-00000-of-00001.parquet\",\n",
    "    repo_id=\"mesolitica/Malaysian-Voice-Conversion\",\n",
    "    repo_type=\"dataset\",\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.10",
   "language": "python",
   "name": "python3.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
