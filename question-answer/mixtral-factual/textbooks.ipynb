{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c73e8d15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/husein/.local/lib/python3.8/site-packages/requests/__init__.py:102: RequestsDependencyWarning: urllib3 (1.26.15) or chardet (5.2.0)/charset_normalizer (2.0.7) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({})/charset_normalizer ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import InferenceClient\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3dbfbcaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "generate_kwargs = dict(\n",
    "    temperature=1.0,\n",
    "    max_new_tokens=4096,\n",
    "    top_p=0.95,\n",
    "    repetition_penalty=1.0,\n",
    "    do_sample=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a98c43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = InferenceClient(\n",
    "    \"https://mixtral.us-west-2.mesolitica.com\", timeout = 120\n",
    ")\n",
    "\n",
    "\n",
    "def format_prompt(message, history):\n",
    "  prompt = \"<s>\"\n",
    "  for user_prompt, bot_response in history:\n",
    "    prompt += f\"[INST] {user_prompt} [/INST]\"\n",
    "    prompt += f\" {bot_response}</s> \"\n",
    "  prompt += f\"[INST] {message} [/INST]\"\n",
    "  return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34d9a695",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/home/husein/.cache/huggingface/datasets/open-phi___parquet/open-phi--textbooks-b1f9998a547cd367/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "438832a7f7b74699934247db3e7f866b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"open-phi/textbooks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ff86e92",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['topic', 'model', 'concepts', 'outline', 'markdown', 'field', 'subfield', 'rag'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2cba95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def partition(text, size = 2000):\n",
    "    splitted = text.split()\n",
    "    return [' '.join(splitted[i: i + size]) for i in range(0, len(splitted), size) if len(splitted[i: i + size]) > 500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "351eff82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28809"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "partitions = []\n",
    "for i in range(len(dataset['train'])):\n",
    "    partitions.extend(partition(dataset['train'][i]['markdown']))\n",
    "    \n",
    "len(partitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9372dfaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "partitions = partitions * 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5bf8110e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "86427"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompts = []\n",
    "for t in partitions:\n",
    "    prompt = f'-------\\n{t}\\n-------\\n\\ngenerate factually incorrect and confusing questions ONLY to trick people based on context above'\n",
    "    prompts.extend([(t, prompt)] * 1)\n",
    "    \n",
    "len(prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "85a97021",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘mixtral-rag-question-factually-wrong-textbook’: File exists\r\n"
     ]
    }
   ],
   "source": [
    "!mkdir mixtral-rag-question-factually-wrong-textbook\n",
    "# !rm mixtral-rag-question-factually-wrong-textbook/*.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c490909c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer(q, i):\n",
    "    filename = f'mixtral-rag-question-factually-wrong-textbook/{i}.json'\n",
    "    if os.path.exists(filename):\n",
    "        return\n",
    "    \n",
    "    \n",
    "    for _ in range(3):\n",
    "        try:\n",
    "            prompt = q[1]\n",
    "            formatted_prompt = format_prompt(prompt, [])\n",
    "            stream = client.text_generation(formatted_prompt, **generate_kwargs, stream=False, details=True, return_full_text=False)\n",
    "            output = stream.generated_text\n",
    "            splitted = output.split('\\n')\n",
    "            splitted = [s for s in splitted if len(s) > 3]\n",
    "            \n",
    "            if len(splitted) < 4:\n",
    "                continue\n",
    "                \n",
    "            with open(filename, 'w') as fopen:\n",
    "                json.dump((q[0], output), fopen)\n",
    "            break\n",
    "        except Exception as e:\n",
    "            # print(e)\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8dd02e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer(prompts[0], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "02038966",
   "metadata": {},
   "outputs": [],
   "source": [
    "def consumer(queue, name):\n",
    "    while True:\n",
    "        if queue.qsize() == 0:\n",
    "            break\n",
    "        item = queue.get()\n",
    "        answer(*item)\n",
    "    print(f'consumer {name} done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e256fd74",
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = [(q, no) for no, q in enumerate(prompts)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "22d63b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from threading import Thread\n",
    "from queue import Queue\n",
    "\n",
    "queue = Queue()\n",
    "for u in urls:\n",
    "    queue.put(u)\n",
    "    \n",
    "ori_size = queue.qsize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "74bede37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|████████████████████████████████████████████████████████████████████████████▎                             | 62259/86427 [7:19:58<3:09:20,  2.13it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "max_worker = 256\n",
    "consumers = [Thread(target=consumer, args=(queue,i)) for i in range(max_worker)]\n",
    "for i in range(len(consumers)):\n",
    "    consumers[i].start()\n",
    "    \n",
    "pbar = tqdm(total=ori_size)\n",
    "last_size = 0\n",
    "while True:\n",
    "    size = queue.qsize()\n",
    "    if size == 0:\n",
    "        break\n",
    "    left = ori_size - size\n",
    "    minus = left - last_size\n",
    "    if minus > 0:\n",
    "        pbar.update(minus)\n",
    "        last_size += minus\n",
    "\n",
    "pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7873e75c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
