{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa088073",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "from transformers import AutoTokenizer, WhisperConfig\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "config = WhisperConfig.from_pretrained('openai/whisper-large-v3')\n",
    "maxlen = config.max_length - 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c431b5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('openai/whisper-large-v3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a39b6491",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Audio\n",
    "\n",
    "sr = 16000\n",
    "audio = Audio(sampling_rate=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1db5783c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38052"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files = sorted(glob('output-malaya/*.json'), key = lambda x: int(x.split('-')[-1].replace('.json', '')))\n",
    "len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a02855e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'output-malaya/2-0.json'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e26c008d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(files[0]) as fopen:\n",
    "    d = json.load(fopen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cfd6657f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|startoftranscript|><|ms|><|transcribe|><|0.00|> kerajaan persekutuan<|1.46|><|1.46|> dan banyak masalah hubungan<|3.96|><|3.96|> antara kerajaan negeri dan<|5.70|><|endoftext|>'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array(d[0]['predict_ms'])\n",
    "a = a[a != 50257].tolist() + [50257]\n",
    "t = tokenizer.decode(a, decode_with_timestamps = True).strip()\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b5e9a473",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('0.00', ' kerajaan persekutuan', '1.46'),\n",
       " ('1.46', ' dan banyak masalah hubungan', '3.96')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "pattern_pair = r'<\\|(\\d+\\.\\d+)\\|>(.*?)<\\|(\\d+\\.\\d+)\\|>'\n",
    "matches = re.findall(pattern_pair, '<|0.00|> kerajaan persekutuan<|1.46|><|1.46|> dan banyak masalah hubungan<|3.96|><|3.96|> antara kerajaan negeri dan')\n",
    "matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d39c5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "punct = set(string.punctuation)\n",
    "\n",
    "def remove_punct(s):\n",
    "    return ''.join([c for c in s if c not in punct])\n",
    "\n",
    "def remove_duplicate(string, n = 3):\n",
    "    splitted = string.split()\n",
    "    n = [splitted[i: i + n] for i in range(0, len(splitted), n)]\n",
    "    already = set()\n",
    "    dedup = []\n",
    "    for n_ in n:\n",
    "        original_n = ' '.join(n_)\n",
    "        n_ = ' '.join(n_).lower()\n",
    "        n_ = remove_punct(n_)\n",
    "        if n_ not in already:\n",
    "            dedup.append(original_n)\n",
    "            already.add(n_)\n",
    "    return ' '.join(dedup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c2839f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def round_to_nearest_0_02(number):\n",
    "    return round(number * 50) / 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "43e960ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected = [\n",
    "    'terima kasih kerana menonton',\n",
    "    'terima kasih',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9551e7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mp\n",
    "import copy\n",
    "\n",
    "minimum_score = 5\n",
    "\n",
    "def loop(files):\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained('openai/whisper-large-v3')\n",
    "    \n",
    "    files, _ = files\n",
    "    results = []\n",
    "    for f in tqdm(files):\n",
    "        try:\n",
    "            with open(f) as fopen:\n",
    "                data = json.load(fopen)\n",
    "        except:\n",
    "            continue\n",
    "        f_split = os.path.split(f)[-1].replace('.json', '')\n",
    "        for i in range(len(data)):\n",
    "            \n",
    "            audio_filename = os.path.join('output-audio-malaya', f'{f_split}-{i}.mp3')\n",
    "            if not os.path.exists(audio_filename):\n",
    "                continue\n",
    "                \n",
    "            y = audio.decode_example(audio.encode_example(audio_filename))['array']\n",
    "            len_y = len(y) / sr\n",
    "            if len_y > 30:\n",
    "                continue\n",
    "            rounded_num = f'<|{round_to_nearest_0_02(len_y):.2f}|>'\n",
    "            \n",
    "            if data[i]['score_ms'] > minimum_score:\n",
    "                a = np.array(data[i]['predict_ms'])\n",
    "                a = a[a != 50257].tolist() + [50257]\n",
    "                t = tokenizer.decode(a, skip_special_tokens = True, decode_with_timestamps = True).strip()\n",
    "                if t.split('|>')[-1] != '':\n",
    "                    t += rounded_num\n",
    "                \n",
    "                matches = re.findall(pattern_pair, t)\n",
    "                rs = []\n",
    "                for match in matches:\n",
    "                    l = float(match[0])\n",
    "                    r = float(match[2])\n",
    "                    t_ = match[1]\n",
    "                    rt_ = re.sub('[^a-z ]+', '', t_.lower()).strip()\n",
    "                    if (r - l > 3) and any([s == rt_ for s in selected]):\n",
    "                        # print(audio_filename, t_)\n",
    "                        t_ = ''\n",
    "                    else:\n",
    "                        try:\n",
    "                            dense = CountVectorizer(ngram_range = (3,3)).fit_transform([t_]).todense()\n",
    "                            repeat = (dense > 3).sum() >= 1\n",
    "                            if repeat:\n",
    "                                t_ = remove_duplicate(t_)\n",
    "                        except:\n",
    "                            if len(t_) > 100:\n",
    "                                t_ = remove_duplicate(t_)\n",
    "                    rs.append(f'<|{match[0]}|>{t_}<|{match[2]}|>')\n",
    "                rs = ''.join(rs)\n",
    "                t = f'<|startoftranscript|><|ms|><|transcribe|>{rs}<|endoftext|>'\n",
    "                d = {\n",
    "                    'new_text': t,\n",
    "                    'audio_filename': audio_filename,\n",
    "                }\n",
    "                results.append(d)\n",
    "                    \n",
    "            \n",
    "            if data[i]['score_en'] > minimum_score:\n",
    "                a = np.array(data[i]['predict_en'])\n",
    "                a = a[a != 50257].tolist() + [50257]\n",
    "                t = tokenizer.decode(a, skip_special_tokens = True, decode_with_timestamps = True).strip()\n",
    "                if t.split('|>')[-1] != '':\n",
    "                    t += rounded_num\n",
    "                \n",
    "                matches = re.findall(pattern_pair, t)\n",
    "                rs = []\n",
    "                for match in matches:\n",
    "                    l = float(match[0])\n",
    "                    r = float(match[2])\n",
    "                    t_ = match[1]\n",
    "                    rt_ = re.sub('[^a-z ]+', '', t_.lower()).strip()\n",
    "                    if (r - l > 3) and any([s == rt_ for s in selected]):\n",
    "                        # print(audio_filename, t_)\n",
    "                        t_ = ''\n",
    "                    else:\n",
    "                        try:\n",
    "                            dense = CountVectorizer(ngram_range = (3,3)).fit_transform([t_]).todense()\n",
    "                            repeat = (dense > 3).sum() >= 1\n",
    "                            if repeat:\n",
    "                                t_ = remove_duplicate(t_)\n",
    "                        except:\n",
    "                            if len(t_) > 100:\n",
    "                                t_ = remove_duplicate(t_)\n",
    "                    rs.append(f'<|{match[0]}|>{t_}<|{match[2]}|>')\n",
    "                rs = ''.join(rs)\n",
    "                t = f'<|startoftranscript|><|en|><|transcribe|>{rs}<|endoftext|>'\n",
    "                d = {\n",
    "                    'new_text': t,\n",
    "                    'audio_filename': audio_filename,\n",
    "                }\n",
    "                results.append(d)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7abad3a5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "100%|██████████| 10/10 [00:16<00:00,  1.64s/it]\n"
     ]
    }
   ],
   "source": [
    "results = loop((files[:10], 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e9f8c38d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "  0%|          | 0/380 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "  0%|          | 1/380 [00:00<03:40,  1.72it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "  0%|          | 0/380 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "  0%|          | 0/380 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "  0%|          | 0/380 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "  0%|          | 1/380 [00:00<03:20,  1.89it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "  0%|          | 0/380 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "  0%|          | 0/380 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "  0%|          | 1/380 [00:00<02:59,  2.11it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "  0%|          | 0/380 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "  0%|          | 1/380 [00:00<04:06,  1.54it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "  0%|          | 0/380 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "  0%|          | 0/380 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "  0%|          | 0/380 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "  0%|          | 0/380 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "  0%|          | 1/380 [00:01<07:43,  1.22s/it]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "  0%|          | 0/380 [00:00<?, ?it/s].05it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "  0%|          | 0/380 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "  0%|          | 0/380 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "  0%|          | 0/380 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "  0%|          | 0/380 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "  0%|          | 0/380 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "  0%|          | 0/380 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "  0%|          | 0/380 [00:00<?, ?it/s].05it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "  0%|          | 0/380 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "  0%|          | 0/380 [00:00<?, ?it/s].59s/it]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "  0%|          | 0/380 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "  0%|          | 0/380 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "  0%|          | 0/380 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "  0%|          | 0/380 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "  0%|          | 0/380 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "  0%|          | 0/380 [00:00<?, ?it/s].83it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "  0%|          | 0/380 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "  0%|          | 0/380 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "  0%|          | 0/380 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "  0%|          | 0/380 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "  0%|          | 0/380 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "  0%|          | 0/380 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "  0%|          | 0/380 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "  0%|          | 0/380 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "  0%|          | 0/380 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "  0%|          | 0/380 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "  0%|          | 0/380 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "  1%|          | 2/380 [00:00<02:32,  2.48it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "  0%|          | 0/380 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "  0%|          | 0/380 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "  0%|          | 0/380 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "  0%|          | 0/380 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "  0%|          | 0/380 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "  0%|          | 0/380 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "  0%|          | 1/380 [00:02<12:40,  2.01s/it]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "  0%|          | 0/380 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "  0%|          | 0/380 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "  0%|          | 0/380 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "  0%|          | 0/380 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "  0%|          | 0/380 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "  0%|          | 0/380 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "  0%|          | 0/380 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "  0%|          | 0/380 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "  0%|          | 0/380 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "  0%|          | 0/380 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "  0%|          | 0/380 [00:00<?, ?it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "  1%|          | 3/380 [00:01<03:11,  1.96it/s]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "100%|██████████| 380/380 [1:09:16<00:00, 10.94s/it]\n",
      "100%|██████████| 380/380 [1:09:16<00:00, 10.94s/it]\n",
      " 95%|█████████▌| 362/380 [1:09:17<03:39, 12.18s/it]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "100%|██████████| 380/380 [1:09:23<00:00, 10.96s/it]\n",
      "100%|██████████| 380/380 [1:09:32<00:00, 10.98s/it]\n",
      "100%|██████████| 380/380 [1:09:48<00:00, 11.02s/it]\n",
      "100%|██████████| 380/380 [1:09:50<00:00, 11.03s/it]\n",
      "100%|██████████| 380/380 [1:09:55<00:00, 11.04s/it]\n",
      "100%|██████████| 380/380 [1:09:57<00:00, 11.05s/it]\n",
      "100%|██████████| 380/380 [1:09:55<00:00, 11.04s/it]\n",
      "100%|██████████| 380/380 [1:10:01<00:00, 11.06s/it]\n",
      "100%|██████████| 380/380 [1:10:01<00:00, 11.06s/it]\n",
      "100%|██████████| 380/380 [1:10:01<00:00, 11.06s/it]\n",
      "100%|██████████| 380/380 [1:10:06<00:00, 11.07s/it]\n",
      "100%|██████████| 380/380 [1:10:08<00:00, 11.07s/it]\n",
      "100%|██████████| 380/380 [1:10:12<00:00, 11.09s/it]\n",
      "100%|██████████| 380/380 [1:10:14<00:00, 11.09s/it]\n",
      "100%|██████████| 380/380 [1:10:15<00:00, 11.09s/it]\n",
      "100%|██████████| 380/380 [1:10:20<00:00, 11.11s/it]\n",
      "100%|██████████| 380/380 [1:10:20<00:00, 11.11s/it]\n",
      "100%|██████████| 380/380 [1:10:22<00:00, 11.11s/it]\n",
      "100%|██████████| 380/380 [1:10:23<00:00, 11.11s/it]\n",
      "100%|██████████| 380/380 [1:10:21<00:00, 11.11s/it]\n",
      "100%|██████████| 380/380 [1:10:24<00:00, 11.12s/it]\n",
      "100%|██████████| 380/380 [1:10:24<00:00, 11.12s/it]\n",
      "100%|██████████| 380/380 [1:10:27<00:00, 11.12s/it]\n",
      "100%|██████████| 380/380 [1:10:30<00:00, 11.13s/it]\n",
      "100%|██████████| 380/380 [1:10:32<00:00, 11.14s/it]\n",
      "100%|██████████| 380/380 [1:10:34<00:00, 11.14s/it]\n",
      "100%|██████████| 380/380 [1:10:32<00:00, 11.14s/it]\n",
      "100%|██████████| 380/380 [1:10:36<00:00, 11.15s/it]\n",
      "100%|██████████| 380/380 [1:10:34<00:00, 11.14s/it]\n",
      "100%|██████████| 380/380 [1:10:38<00:00, 11.15s/it]\n",
      "100%|██████████| 380/380 [1:10:37<00:00, 11.15s/it]\n",
      "100%|██████████| 380/380 [1:10:40<00:00, 11.16s/it]\n",
      "100%|██████████| 380/380 [1:10:41<00:00, 11.16s/it]\n",
      "100%|██████████| 380/380 [1:10:42<00:00, 11.17s/it]\n",
      "100%|██████████| 380/380 [1:10:43<00:00, 11.17s/it]\n",
      "100%|██████████| 380/380 [1:10:44<00:00, 11.17s/it]\n",
      "100%|██████████| 380/380 [1:10:48<00:00, 11.18s/it]\n",
      "100%|██████████| 380/380 [1:10:49<00:00, 11.18s/it]\n",
      "100%|██████████| 380/380 [1:10:47<00:00, 11.18s/it]\n",
      "100%|██████████| 380/380 [1:10:48<00:00, 11.18s/it]\n",
      "100%|██████████| 380/380 [1:10:51<00:00, 11.19s/it]\n",
      "100%|██████████| 380/380 [1:10:53<00:00, 11.19s/it]\n",
      "100%|██████████| 380/380 [1:10:54<00:00, 11.20s/it]\n",
      "100%|██████████| 380/380 [1:10:53<00:00, 11.19s/it]\n",
      "100%|██████████| 380/380 [1:10:53<00:00, 11.19s/it]\n",
      "100%|██████████| 380/380 [1:10:54<00:00, 11.20s/it]\n",
      "100%|██████████| 380/380 [1:10:55<00:00, 11.20s/it]\n",
      "100%|██████████| 380/380 [1:10:56<00:00, 11.20s/it]\n",
      "100%|██████████| 380/380 [1:10:57<00:00, 11.20s/it]\n",
      "100%|██████████| 380/380 [1:10:57<00:00, 11.20s/it]\n",
      "100%|██████████| 380/380 [1:10:58<00:00, 11.21s/it]\n",
      "100%|██████████| 380/380 [1:11:02<00:00, 11.22s/it]\n",
      "100%|██████████| 380/380 [1:11:05<00:00, 11.23s/it]\n",
      "100%|██████████| 380/380 [1:11:05<00:00, 11.22s/it]\n",
      "100%|██████████| 380/380 [1:11:03<00:00, 11.22s/it]\n",
      "100%|██████████| 380/380 [1:11:04<00:00, 11.22s/it]\n",
      "100%|██████████| 380/380 [1:11:04<00:00, 11.22s/it]\n",
      "100%|██████████| 380/380 [1:11:06<00:00, 11.23s/it]\n",
      "100%|██████████| 380/380 [1:11:07<00:00, 11.23s/it]\n",
      "100%|██████████| 380/380 [1:11:06<00:00, 11.23s/it]\n",
      "100%|██████████| 380/380 [1:11:08<00:00, 11.23s/it]\n",
      "100%|██████████| 380/380 [1:11:09<00:00, 11.24s/it]\n",
      "100%|██████████| 380/380 [1:11:10<00:00, 11.24s/it]\n",
      "100%|██████████| 380/380 [1:11:09<00:00, 11.24s/it]\n",
      "100%|██████████| 380/380 [1:11:09<00:00, 11.24s/it]\n",
      "100%|██████████| 380/380 [1:11:11<00:00, 11.24s/it]\n",
      "100%|██████████| 380/380 [1:11:11<00:00, 11.24s/it]\n",
      "100%|██████████| 380/380 [1:11:11<00:00, 11.24s/it]\n",
      "100%|██████████| 380/380 [1:11:11<00:00, 11.24s/it]\n",
      "100%|██████████| 380/380 [1:11:12<00:00, 11.24s/it]\n",
      "100%|██████████| 380/380 [1:11:13<00:00, 11.25s/it]\n",
      "100%|██████████| 380/380 [1:11:13<00:00, 11.24s/it]\n",
      "100%|██████████| 380/380 [1:11:16<00:00, 11.25s/it]\n",
      "100%|██████████| 380/380 [1:11:17<00:00, 11.26s/it]\n",
      "100%|██████████| 380/380 [1:11:15<00:00, 11.25s/it]\n",
      "100%|██████████| 380/380 [1:11:15<00:00, 11.25s/it]\n",
      "100%|██████████| 380/380 [1:11:16<00:00, 11.25s/it]\n",
      "100%|██████████| 380/380 [1:11:17<00:00, 11.26s/it]\n",
      "100%|██████████| 380/380 [1:11:19<00:00, 11.26s/it]\n",
      "100%|██████████| 380/380 [1:11:18<00:00, 11.26s/it]\n",
      "100%|██████████| 380/380 [1:11:20<00:00, 11.26s/it]\n",
      "100%|██████████| 380/380 [1:11:21<00:00, 11.27s/it]\n",
      "100%|██████████| 380/380 [1:11:21<00:00, 11.27s/it]\n",
      "100%|██████████| 380/380 [1:11:21<00:00, 11.27s/it]\n",
      "100%|██████████| 380/380 [1:11:22<00:00, 11.27s/it]\n",
      "100%|██████████| 380/380 [1:11:22<00:00, 11.27s/it]\n",
      "100%|██████████| 380/380 [1:11:25<00:00, 11.28s/it]\n",
      "100%|██████████| 380/380 [1:11:22<00:00, 11.27s/it]\n",
      "100%|██████████| 380/380 [1:11:22<00:00, 11.27s/it]\n",
      "100%|██████████| 380/380 [1:11:25<00:00, 11.28s/it]\n",
      "100%|██████████| 380/380 [1:11:23<00:00, 11.27s/it]\n",
      "100%|██████████| 380/380 [1:11:24<00:00, 11.28s/it]\n",
      "100%|██████████| 380/380 [1:11:23<00:00, 11.27s/it]\n",
      "100%|██████████| 380/380 [1:11:24<00:00, 11.27s/it]\n",
      "100%|██████████| 380/380 [1:11:24<00:00, 11.27s/it]\n",
      "100%|██████████| 380/380 [1:11:24<00:00, 11.28s/it]\n",
      "100%|██████████| 380/380 [1:11:27<00:00, 11.28s/it]\n",
      "100%|██████████| 380/380 [1:11:28<00:00, 11.29s/it]\n",
      "100%|██████████| 52/52 [02:35<00:00,  2.99s/it]\n"
     ]
    }
   ],
   "source": [
    "results = mp.multiprocessing(files, loop, cores = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f33581b9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1089630"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b0bb47ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'new_text': \"<|startoftranscript|><|en|><|transcribe|><|0.00|> Allah Ta'ala<|1.44|><|1.44|> Jadikan kita<|2.34|><|2.34|> Kita bahasa Melayu<|5.20|><|5.20|> Sama ada bahasa Cina<|6.10|><|endoftext|>\",\n",
       " 'audio_filename': 'output-audio-malaya/0-9735-38.mp3'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[-4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dbbd91b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1089630/1089630 [00:03<00:00, 273327.40it/s]\n"
     ]
    }
   ],
   "source": [
    "with open('prepared-pseudolabel-malaya.jsonl', 'w') as fopen:\n",
    "    for r in tqdm(results):\n",
    "        fopen.write(f'{json.dumps(r)}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fa611004",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi\n",
    "api = HfApi()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "914c3f55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f074ef56ad148379a44356a466757b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "prepared-pseudolabel-malaya.jsonl:   0%|          | 0.00/304M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'https://huggingface.co/datasets/mesolitica/pseudolabel-malaya-speech-stt-train-whisper-large-v3-timestamp/blob/main/prepared-pseudolabel-malaya.jsonl'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "api.upload_file(\n",
    "    path_or_fileobj='prepared-pseudolabel-malaya.jsonl',\n",
    "    path_in_repo='prepared-pseudolabel-malaya.jsonl',\n",
    "    repo_id='mesolitica/pseudolabel-malaya-speech-stt-train-whisper-large-v3-timestamp',\n",
    "    repo_type='dataset',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bbc83727",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('openai/whisper-medium')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c1c4bab9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<|startoftranscript|>',\n",
       " '<|en|>',\n",
       " '<|transcribe|>',\n",
       " '<|0.00|>',\n",
       " 'ĠAllah',\n",
       " 'ĠTa',\n",
       " \"'\",\n",
       " 'ala',\n",
       " '<|1.44|>',\n",
       " '<|1.44|>',\n",
       " 'ĠJ',\n",
       " 'ad',\n",
       " 'ikan',\n",
       " 'Ġkita',\n",
       " '<|2.34|>',\n",
       " '<|2.34|>',\n",
       " 'ĠKita',\n",
       " 'Ġbah',\n",
       " 'asa',\n",
       " 'ĠMel',\n",
       " 'ay',\n",
       " 'u',\n",
       " '<|5.20|>',\n",
       " '<|5.20|>',\n",
       " 'ĠS',\n",
       " 'ama',\n",
       " 'Ġada',\n",
       " 'Ġbah',\n",
       " 'asa',\n",
       " 'ĠC',\n",
       " 'ina',\n",
       " '<|6.10|>',\n",
       " '<|endoftext|>']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(results[-4]['new_text'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
